{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALolaoragulP"
      },
      "source": [
        "# Imports, api, datasets and functions to use Naswot metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK9a6n44hGK6",
        "outputId": "21dfed4b-87ce-4f61-8313-ff08ea758e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nats_bench\n",
            "  Downloading nats_bench-1.7-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from nats_bench) (1.21.6)\n",
            "Installing collected packages: nats-bench\n",
            "Successfully installed nats-bench-1.7\n",
            "--2022-06-26 17:22:53--  https://www.dropbox.com/sh/ceeo70u1buow681/AADxyCvBAnE6mMjp7JOoo0LVa/NATS-tss-v1_0-3ffb9-simple.tar\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /sh/raw/ceeo70u1buow681/AADxyCvBAnE6mMjp7JOoo0LVa/NATS-tss-v1_0-3ffb9-simple.tar [following]\n",
            "--2022-06-26 17:22:53--  https://www.dropbox.com/sh/raw/ceeo70u1buow681/AADxyCvBAnE6mMjp7JOoo0LVa/NATS-tss-v1_0-3ffb9-simple.tar\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc876039a48058814224ef39a483.dl.dropboxusercontent.com/cd/0/inline/Bn_5Qaagz95omuh-QbflumgrzuMcieR_pfo7cUOavPdgO9smQJ8hlthSNsCbM56dycm2ADoKdvPNqCn_T1TgaOZE8aH2uv7CMegUakJ6R5okXCVzX3Ytzdazk8SzdRUqrXzyt25lOZmqFUwQ8QQuinozO6-hJ1xSslC4ub5sx_3WVA/file# [following]\n",
            "--2022-06-26 17:22:54--  https://uc876039a48058814224ef39a483.dl.dropboxusercontent.com/cd/0/inline/Bn_5Qaagz95omuh-QbflumgrzuMcieR_pfo7cUOavPdgO9smQJ8hlthSNsCbM56dycm2ADoKdvPNqCn_T1TgaOZE8aH2uv7CMegUakJ6R5okXCVzX3Ytzdazk8SzdRUqrXzyt25lOZmqFUwQ8QQuinozO6-hJ1xSslC4ub5sx_3WVA/file\n",
            "Resolving uc876039a48058814224ef39a483.dl.dropboxusercontent.com (uc876039a48058814224ef39a483.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc876039a48058814224ef39a483.dl.dropboxusercontent.com (uc876039a48058814224ef39a483.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bn8TgBEFMG-kNEoVQLChX-y0rflL9jd6m_d0rOnTB0s-qCEJMuXLiJmVLvBJsPrKWH398Oc1sD9JI3iLgqpqh15b_4ewptNtILt29_IhZT3isU0Tg7F9LucVL9E5OZc7T-9KNaLA8iYWbtbFholjDmXX4qt-q9g-YS4E6CSwGRKO93FjP8Lw--cHr78q5lyit8Se8t4jNXO5Z9Lig7TgJk3hszyYyYLoYeWX4Ku9TigS-Eln4w19o7yM2YTTCTk7SqyeeAeNwEH-tRDyopjo2cX_7AvRIDanmkltVzPOBXn6iyLPX9pc_dYlIAQABhmJYHmnDudBNdFZDcwuwo8mYd4cYzynFhqgy447gD3tl689zhlu83cw6G_IPQeR7bwCu4Bm0u4CaoZH-hIBQ72sUQKS-ABmhDWL9J_Isyeysi6R3g/file [following]\n",
            "--2022-06-26 17:22:54--  https://uc876039a48058814224ef39a483.dl.dropboxusercontent.com/cd/0/inline2/Bn8TgBEFMG-kNEoVQLChX-y0rflL9jd6m_d0rOnTB0s-qCEJMuXLiJmVLvBJsPrKWH398Oc1sD9JI3iLgqpqh15b_4ewptNtILt29_IhZT3isU0Tg7F9LucVL9E5OZc7T-9KNaLA8iYWbtbFholjDmXX4qt-q9g-YS4E6CSwGRKO93FjP8Lw--cHr78q5lyit8Se8t4jNXO5Z9Lig7TgJk3hszyYyYLoYeWX4Ku9TigS-Eln4w19o7yM2YTTCTk7SqyeeAeNwEH-tRDyopjo2cX_7AvRIDanmkltVzPOBXn6iyLPX9pc_dYlIAQABhmJYHmnDudBNdFZDcwuwo8mYd4cYzynFhqgy447gD3tl689zhlu83cw6G_IPQeR7bwCu4Bm0u4CaoZH-hIBQ72sUQKS-ABmhDWL9J_Isyeysi6R3g/file\n",
            "Reusing existing connection to uc876039a48058814224ef39a483.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1145989120 (1.1G) [application/x-tar]\n",
            "Saving to: ‘NATS-tss-v1_0-3ffb9-simple.tar’\n",
            "\n",
            "NATS-tss-v1_0-3ffb9 100%[===================>]   1.07G  15.9MB/s    in 74s     \n",
            "\n",
            "2022-06-26 17:24:09 (14.7 MB/s) - ‘NATS-tss-v1_0-3ffb9-simple.tar’ saved [1145989120/1145989120]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xautodl\n",
            "  Downloading xautodl-1.0.0-py3-none-any.whl (225 kB)\n",
            "\u001b[K     |████████████████████████████████| 225 kB 33.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.0.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.9 MB/s \n",
            "\u001b[?25hCollecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting numpy<=1.19.5,>=1.16.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 61.3 MB/s \n",
            "\u001b[?25hCollecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->xautodl) (4.64.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->xautodl) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->xautodl) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->xautodl) (0.8.9)\n",
            "Collecting iopath>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=6cd9f335946c0fae3f1c34db5e430988b82c40a38eb7c36c141773b14a9e387a\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\n",
            "Successfully built fvcore\n",
            "Installing collected packages: pyyaml, portalocker, yacs, numpy, iopath, fvcore, xautodl\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed fvcore-0.1.5.post20220512 iopath-0.1.9 numpy-1.19.5 portalocker-2.4.0 pyyaml-6.0 xautodl-1.0.0 yacs-0.1.8\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install nats_bench\n",
        "!wget \"https://www.dropbox.com/sh/ceeo70u1buow681/AADxyCvBAnE6mMjp7JOoo0LVa/NATS-tss-v1_0-3ffb9-simple.tar\"\n",
        "!tar -xf \"NATS-tss-v1_0-3ffb9-simple.tar\"\n",
        "\n",
        "\n",
        "!pip install xautodl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOHcpZQ5raC0",
        "outputId": "83100ec3-9df6-488d-a532-b0bfc3423518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-10 08:43:16--  https://www.dropbox.com/s/o2fg17ipz57nru1/?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/o2fg17ipz57nru1 [following]\n",
            "--2022-06-10 08:43:16--  https://www.dropbox.com/s/dl/o2fg17ipz57nru1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1ad5f1e91ecd2f608ef47c408b.dl-eu.dropboxusercontent.com/cd/0/get/Bm5pV_v6GC_qCygUjfHGcX1s_CchUV8zC0F9wtkz2d6XCw4lihbF4VMjm7A6S3iQADs8cevJigob4n06BdEsQrlLfFkgaGIHKhrHkkuYRmijbvDdaHJI1vyIVhf-Jad3sXOw0npQJdRrt1wSUOY9BkT_EKEtgHbpdIQFYGyc1Qv-PQ7y216Zl94OAPXDhTdpNKc/file?dl=1# [following]\n",
            "--2022-06-10 08:43:17--  https://uc1ad5f1e91ecd2f608ef47c408b.dl-eu.dropboxusercontent.com/cd/0/get/Bm5pV_v6GC_qCygUjfHGcX1s_CchUV8zC0F9wtkz2d6XCw4lihbF4VMjm7A6S3iQADs8cevJigob4n06BdEsQrlLfFkgaGIHKhrHkkuYRmijbvDdaHJI1vyIVhf-Jad3sXOw0npQJdRrt1wSUOY9BkT_EKEtgHbpdIQFYGyc1Qv-PQ7y216Zl94OAPXDhTdpNKc/file?dl=1\n",
            "Resolving uc1ad5f1e91ecd2f608ef47c408b.dl-eu.dropboxusercontent.com (uc1ad5f1e91ecd2f608ef47c408b.dl-eu.dropboxusercontent.com)... 162.125.80.15, 2620:100:6035:15::a27d:550f\n",
            "Connecting to uc1ad5f1e91ecd2f608ef47c408b.dl-eu.dropboxusercontent.com (uc1ad5f1e91ecd2f608ef47c408b.dl-eu.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 967949552 (923M) [application/binary]\n",
            "Saving to: ‘ImageNet16.tar.gz’\n",
            "\n",
            "ImageNet16.tar.gz   100%[===================>] 923.11M  10.6MB/s    in 98s     \n",
            "\n",
            "2022-06-10 08:44:57 (9.37 MB/s) - ‘ImageNet16.tar.gz’ saved [967949552/967949552]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://www.dropbox.com/s/o2fg17ipz57nru1/?dl=1' -O ImageNet16.tar.gz\n",
        "!tar -xf \"ImageNet16.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fbLfcgRhGK9"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from xautodl.datasets.DownsampledImageNet import ImageNet16\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch\n",
        "import operator\n",
        "from numba import cuda\n",
        "from nats_bench import create\n",
        "from xautodl.models import get_cell_based_tiny_net\n",
        "import random\n",
        "import pandas as pd\n",
        "import os\n",
        "from scipy import stats\n",
        "import time\n",
        "import operator\n",
        "import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zkHxjuIdqMm"
      },
      "outputs": [],
      "source": [
        "# Create the API instance for the size search space in NATS\n",
        "api = create(\"/content/NATS-tss-v1_0-3ffb9-simple/\", 'tss', fast_mode=True, verbose=False)\n",
        "\n",
        "#tranforms to tensor the input data, no data augmentation needed\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "#funtion to retrieve the train loader\n",
        "def get_data(dataset, batch_size, pin_memory=True):\n",
        "  transform = transforms.Compose([\n",
        "    transforms.ToTensor() ])\n",
        "  if dataset == 'cifar10':\n",
        "    train_data = dset.CIFAR10 (\"/content/Cifar10\", train=True ,transform = transform, download=True)\n",
        "\n",
        "  elif dataset == 'cifar100':\n",
        "    train_data = dset.CIFAR100(\"/content/Cifar100\", train=True ,transform = transform, download=True)\n",
        "    \n",
        "  elif dataset == 'ImageNet16-120':\n",
        "    train_data = ImageNet16(\"/content/ImageNet16\", True, transform = transform)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=pin_memory)\n",
        "  \n",
        "  return train_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CkHE9SblctR"
      },
      "outputs": [],
      "source": [
        "#log of kernel function\n",
        "def logdet(K):\n",
        "    s, ld = np.linalg.slogdet(K)\n",
        "    return ld\n",
        "\n",
        "#weight initialization function\n",
        "def init(m):\n",
        "    if isinstance(m, (torch.nn.Conv2d, torch.nn.Linear)):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "# function to compute Hamming Distance, using cuda jit library\n",
        "# A is the binary codification of the current Relu output\n",
        "# C is the hamming distance (result of this function) \n",
        "@cuda.jit\n",
        "def constructK(A,C):\n",
        "    i, j = cuda.grid(2)\n",
        "    if i < C.shape[0] and j < C.shape[1]:\n",
        "        tmp = 0.\n",
        "        for k in range(A.shape[1]):\n",
        "            tmp += operator.xor(bool(A[i, k]),bool(A[j, k]))\n",
        "        C[i, j] = tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4btNaLGVz8wu"
      },
      "outputs": [],
      "source": [
        "# hook registered on forward in each Relu instance of the network\n",
        "# here we binarize the output of Relu, setup the parameters to parallelize with cuda.jit\n",
        "# construct the kernel matrix\n",
        "def counting_forward_hook(module, inp, out):\n",
        "  if isinstance(inp, tuple):\n",
        "    inp = inp[0]\n",
        "  inp = inp.view(inp.size(0), -1)\n",
        "  inp = (inp > 0).float()\n",
        "\n",
        "  inp = cuda.to_device(inp)\n",
        "  temp = cuda.device_array((inp.shape[0], inp.shape[0]))\n",
        "\n",
        "  threadsperblock = (8,8)\n",
        "  blockspergrid = (16,16)\n",
        "\n",
        "# Start the kernel \n",
        "  constructK[blockspergrid, threadsperblock](inp, temp)\n",
        "\n",
        "# Copy the result back to the host\n",
        "  global Ktemp, device\n",
        "  temp = torch.tensor(temp.copy_to_host()).to(device)\n",
        "  temp = inp.shape[1]-temp\n",
        "  Ktemp = Ktemp + temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xnYR3nS1B3S"
      },
      "outputs": [],
      "source": [
        "# dictionary that registers isomorphisms, useful to compute score only in the unique models\n",
        "networks = {}\n",
        "ids = []\n",
        "\n",
        "for i in range(len(api)):\n",
        "  # we use get_unique_str() that return a string unique for every isomorphism\n",
        "  net = api.get_unique_str(i)\n",
        "  if net not in networks.keys():\n",
        "    networks[net] = [i]\n",
        "    ids.append(i)\n",
        "  else :\n",
        "    networks[net].append(i)\n",
        "\n",
        "nets = {}\n",
        "for k,v in networks.items():\n",
        "  if len(v) > 1:\n",
        "    nets[v[0]] = v[1:]\n",
        "  else:\n",
        "    nets[v[0]] = []\n",
        "\n",
        "del networks\n",
        "\n",
        "ids = np.array(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCy-izAxhnka"
      },
      "source": [
        "#Step 2a\n",
        "Random search picking the model with highest Naswot metric, considering different sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdhEilu8vPIJ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "datasetNames = ['cifar10','cifaar100','ImageNet16-120']\n",
        "\n",
        "# number of iterations \n",
        "runs_naswot = 10\n",
        "# we consider different sizes of the random sample\n",
        "sample_sizes = [50,100,200,300]\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "for dataset in datasetNames:  \n",
        "  for sample_size in sample_sizes:\n",
        "    for run in range(runs_naswot):\n",
        "\n",
        "      train_loader = get_data(dataset, batch_size)\n",
        "      \n",
        "      # list to trace the scores for the selected networks\n",
        "      scores = []\n",
        "      \n",
        "      nets = np.random.randint(0,len(api),size=sample_size)\n",
        "\n",
        "      start = time.time()\n",
        "\n",
        "      for i in nets:\n",
        "\n",
        "        config = api.get_net_config(i, dataset)\n",
        "        network = get_cell_based_tiny_net(config)\n",
        "        network.apply(init)\n",
        "\n",
        "        for name, module in network.named_modules():\n",
        "          if (isinstance(module, torch.nn.modules.activation.ReLU)):\n",
        "            module.register_forward_hook(counting_forward_hook)\n",
        "\n",
        "        network = network.to(device)\n",
        "\n",
        "        Ktemp = torch.tensor(np.zeros((batch_size,batch_size))).to(device)\n",
        "        data_iterator = iter(train_loader)\n",
        "        x, target = next(data_iterator)\n",
        "        x, target = x.to(device), target.to(device)\n",
        "        network(x)\n",
        "        score = logdet(Ktemp.cpu().detach().numpy(), target)\n",
        "        \n",
        "        del Ktemp, network, data_iterator\n",
        "        \n",
        "        scores.append(score)\n",
        "\n",
        "      #select the best scoring function\n",
        "      ind_best = np.argmax(scores)\n",
        "      acc = api.get_more_info(int(nets[ind_best]),dataset,is_random=False,hp=200)['test-accuracy']\n",
        "      tempo = time.time()-start\n",
        "      csv_dict = {'Dataset': dataset, 'SampleSize' : sample_size,'Network': nets[ind_best], 'Metric': scores[ind_best], 'Accuracy': acc, 'Time': tempo}\n",
        "      result = pd.DataFrame([csv_dict])\n",
        "      result.to_csv('naswot_score.csv', mode='a', index=False, header=False )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdF-JS8mkTkK"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/naswot_score.csv\",names = ['Dataset','size','Network','Metric','Accuracy','Time'], header = None, index_col=['Dataset','Network'])\n",
        "\n",
        "metric = df.groupby(by = ['Dataset','size'])['Metric'].describe()[['mean','std']]\n",
        "accuracy = df.groupby(by = ['Dataset','size'])['Accuracy'].describe()[['mean','std']]\n",
        "time = df.groupby(by = ['Dataset','size'])['Time'].describe()[['mean','std']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q37KQe3LtreI"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([metric,accuracy,time],axis=1)\n",
        "data.to_csv('stats_naswot.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7KpdJg-4e4h"
      },
      "outputs": [],
      "source": [
        "data.columns = ['metric_mean','metirc_std','acc_mean','acc_std','time_mean','time_std']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaHVWutalGHP"
      },
      "source": [
        "#Step 2b\n",
        "Random search picking the model with highest accuracy on testSet after 12 epoches of training, considering different sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F1BO6ewAM0x"
      },
      "outputs": [],
      "source": [
        "datasetNames = ['cifar10','cifaar100','ImageNet16-120']\n",
        "\n",
        "runs_naswot = 10\n",
        "sample_sizes = [50,100,200,300]\n",
        "\n",
        "for dataset in datasetNames:  \n",
        "  for sample_size in sample_sizes:\n",
        "    for run in range(runs_naswot):\n",
        "\n",
        "      # total time to perform these trainings\n",
        "      tempo = 0\n",
        "\n",
        "      start = time.time()\n",
        "\n",
        "      # list to trace the 12 epoch accuracies\n",
        "      accuracies = []\n",
        "      nets = np.random.randint(0,len(api),size=sample_size)\n",
        "\n",
        "      for i in nets:\n",
        "        # get_more_info gives us all the informations we need (accuracy and time to train at 12 epoches)\n",
        "        accuracies.append(api.get_more_info(int(i),dataset,is_random=False,hp=12)['test-accuracy']) \n",
        "        tempo += api.get_more_info(int(i),dataset,is_random=False,hp=12)['train-all-time']\n",
        "\n",
        "      # we take the best network for 12 epoches test accuracy\n",
        "      ind_best = np.argmax(accuracies)\n",
        "      \n",
        "      # consider the 200 epoch test accuracy \n",
        "      final_acc = api.get_more_info(int(nets[ind_best]),dataset,is_random=False,hp=200)['test-accuracy']\n",
        "      tempo += (time.time()-start)\n",
        "      csv_dict = {'Dataset': dataset, 'size':sample_size, 'Network': nets[ind_best], 'Accuracy12': accuracies[ind_best], 'Accuracy200': final_acc, 'Time': tempo}\n",
        "      result = pd.DataFrame([csv_dict])\n",
        "      result.to_csv('naswot_accuracies.csv', mode='a', index=False, header=False )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU1l1iENGMsG"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/naswot_accuracies.csv\",names = ['Dataset','size','Network','Accuracy12','Accuracy200','Time'], header = None, index_col=['Dataset','Network'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HH4y_FDGwVt"
      },
      "outputs": [],
      "source": [
        "accuracy12 = df.groupby(by = ['Dataset','size'])['Accuracy12'].describe()[['mean','std']]\n",
        "accuracy200 = df.groupby(by = ['Dataset','size'])['Accuracy200'].describe()[['mean','std']]\n",
        "time = df.groupby(by = ['Dataset','size'])['Time'].describe()[['mean','std']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLIA5eEOGyRw"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([accuracy12,accuracy200,time],axis=1)\n",
        "data.to_csv('stats_accuracy.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPgGvCC0n3as"
      },
      "outputs": [],
      "source": [
        "data.columns = ['acc12_mean','acc12_std','acc_mean','acc_std','time_mean','time_std']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}